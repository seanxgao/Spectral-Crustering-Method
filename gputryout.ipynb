{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89e6d59d-ca48-417b-8b05-9ca2544da87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuPy is available! GPU acceleration enabled.\n",
      "GPU Memory: 10.79GB free / 12.00GB total\n",
      "GPU threshold set to: 100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import linalg as LA\n",
    "from scipy.sparse import csr_matrix\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys\n",
    "import tracemalloc\n",
    "import gc\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    from cupyx.scipy.sparse import linalg as cupy_LA\n",
    "    from cupyx.scipy.sparse import csr_matrix as cupy_csr_matrix\n",
    "    CUPY_AVAILABLE = True\n",
    "    print(\"CuPy is available! GPU acceleration enabled.\")\n",
    "\n",
    "    mempool = cp.get_default_memory_pool()\n",
    "    pinned_mempool = cp.get_default_pinned_memory_pool()\n",
    "    \n",
    "    # Check GPU memory\n",
    "    free_mem, total_mem = cp.cuda.runtime.memGetInfo()\n",
    "    print(f\"GPU Memory: {free_mem/1024**3:.2f}GB free / {total_mem/1024**3:.2f}GB total\")\n",
    "    \n",
    "    GPU_MIN_SIZE = 100  # Threshold for GPU usage\n",
    "    print(f\"GPU threshold set to: {GPU_MIN_SIZE}\")\n",
    "    \n",
    "except ImportError:\n",
    "    CUPY_AVAILABLE = False\n",
    "    GPU_MIN_SIZE = float('inf')\n",
    "    print(\"CuPy not available. Only CPU version will work.\")\n",
    "\n",
    "# Global verbosity flag - simple boolean\n",
    "VERBOSE = False\n",
    "\n",
    "def set_verbose(verbose=True):\n",
    "    \"\"\"Set global verbosity\"\"\"\n",
    "    global VERBOSE\n",
    "    VERBOSE = verbose\n",
    "\n",
    "# ============================================================================\n",
    "# GPU DATA MANAGER\n",
    "# ============================================================================\n",
    "class GPUDataManager:\n",
    "    \"\"\"\n",
    "    Manages data lifecycle on GPU to minimize CPU-GPU transfers.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.gpu_cache = {}\n",
    "        self.transfer_count = 0\n",
    "        self.transfer_bytes = 0\n",
    "    \n",
    "    def get_gpu_matrix(self, matrix, key=None):\n",
    "        \"\"\"Get matrix on GPU, using cache when possible\"\"\"\n",
    "        if not CUPY_AVAILABLE:\n",
    "            return matrix\n",
    "        \n",
    "        if key and key in self.gpu_cache:\n",
    "            return self.gpu_cache[key]\n",
    "        \n",
    "        if isinstance(matrix, cp.ndarray):\n",
    "            return matrix\n",
    "        \n",
    "        # Transfer to GPU\n",
    "        if hasattr(matrix, 'toarray'):\n",
    "            gpu_matrix = cp.asarray(matrix.toarray())\n",
    "        else:\n",
    "            gpu_matrix = cp.asarray(matrix)\n",
    "        \n",
    "        self.transfer_count += 1\n",
    "        self.transfer_bytes += gpu_matrix.nbytes\n",
    "        \n",
    "        if key:\n",
    "            self.gpu_cache[key] = gpu_matrix\n",
    "        \n",
    "        return gpu_matrix\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear GPU cache and free memory\"\"\"\n",
    "        self.gpu_cache.clear()\n",
    "        if CUPY_AVAILABLE:\n",
    "            mempool = cp.get_default_memory_pool()\n",
    "            mempool.free_all_blocks()\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get transfer statistics\"\"\"\n",
    "        return {\n",
    "            'transfers': self.transfer_count,\n",
    "            'bytes': self.transfer_bytes,\n",
    "            'mb': self.transfer_bytes / (1024**2)\n",
    "        }\n",
    "\n",
    "# Global GPU data manager\n",
    "gpu_manager = GPUDataManager()\n",
    "\n",
    "# ============================================================================\n",
    "# CORE ALGORITHMS\n",
    "# ============================================================================\n",
    "def bicut_group_gpu_native(L_gpu):\n",
    "    \"\"\"\n",
    "    Pure GPU implementation - all operations stay on GPU until final result.\n",
    "    \"\"\"\n",
    "    n = L_gpu.shape[0]\n",
    "    \n",
    "    try:\n",
    "        # Eigenvalue computation on GPU\n",
    "        if cp.sparse.issparse(L_gpu):\n",
    "            _, eigenvecs = cupy_LA.eigsh(L_gpu, k=2, which='SA')\n",
    "        else:\n",
    "            eigenvals, eigenvecs = cp.linalg.eigh(L_gpu)\n",
    "            idx = cp.argsort(eigenvals)\n",
    "            eigenvecs = eigenvecs[:, idx[:2]]\n",
    "        \n",
    "        # Fiedler vector processing - all on GPU\n",
    "        fiedler_vector = eigenvecs[:, 1]\n",
    "        sorted_args = cp.argsort(fiedler_vector)\n",
    "        \n",
    "        # Matrix reordering - stay on GPU\n",
    "        adj = -L_gpu[cp.ix_(sorted_args, sorted_args)]\n",
    "        \n",
    "        # Optimized cut finding - fully vectorized on GPU\n",
    "        if n < 1000:\n",
    "            upper_tri_sums = cp.zeros(n-1)\n",
    "            for i in range(1, n):\n",
    "                upper_tri_sums[i-1] = cp.sum(adj[i:, :i])\n",
    "        else:\n",
    "            # Memory-efficient approach for large matrices\n",
    "            upper_tri_sums = cp.zeros(n-1)\n",
    "            batch_size = min(200, n // 5)\n",
    "            for start in range(1, n, batch_size):\n",
    "                end = min(start + batch_size, n)\n",
    "                for i in range(start, end):\n",
    "                    upper_tri_sums[i-1] = cp.sum(adj[i:, :i])\n",
    "        \n",
    "        # Quality computation on GPU\n",
    "        ind = cp.arange(1, n, dtype=cp.float64)\n",
    "        qualities = upper_tri_sums / (ind * (n - ind))\n",
    "        best_cut = cp.argmin(qualities) + 1\n",
    "        \n",
    "        # Transfer minimal data back to CPU\n",
    "        sorted_args_cpu = cp.asnumpy(sorted_args)\n",
    "        best_cut_cpu = int(cp.asnumpy(best_cut))\n",
    "        \n",
    "        # Final grouping on CPU\n",
    "        first_group = sorted_args_cpu[:best_cut_cpu]\n",
    "        second_group = sorted_args_cpu[best_cut_cpu:]\n",
    "        \n",
    "        if 0 in first_group:\n",
    "            return first_group.tolist(), second_group.tolist()\n",
    "        return second_group.tolist(), first_group.tolist()\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"GPU computation failed: {e}\")\n",
    "\n",
    "def bicut_group(L, use_gpu=False):\n",
    "    \"\"\"\n",
    "    Enhanced spectral clustering with minimal transfer strategy.\n",
    "    \"\"\"\n",
    "    n = L.shape[0]\n",
    "\n",
    "    # Basis steps\n",
    "    if n == 0:\n",
    "        raise ValueError(\"The Laplacian matrix is empty.\") \n",
    "    if n == 1:\n",
    "        return [0], []\n",
    "    if n == 2:\n",
    "        return [0], [1]\n",
    "    \n",
    "    # Decision: Use GPU for matrices above threshold\n",
    "    should_use_gpu = use_gpu and CUPY_AVAILABLE and n >= GPU_MIN_SIZE\n",
    "    \n",
    "    if should_use_gpu:\n",
    "        try:\n",
    "            # Transfer to GPU once\n",
    "            L_gpu = gpu_manager.get_gpu_matrix(L)\n",
    "            return bicut_group_gpu_native(L_gpu)\n",
    "        except Exception as e:\n",
    "            if VERBOSE:\n",
    "                print(f\"GPU failed: {str(e)[:50]}..., using CPU fallback\")\n",
    "            return _bicut_group_cpu(L)\n",
    "    else:\n",
    "        return _bicut_group_cpu(L)\n",
    "\n",
    "def _bicut_group_cpu(L):\n",
    "    \"\"\"Original CPU implementation\"\"\"\n",
    "    n = L.shape[0]\n",
    "    \n",
    "    # Handle CuPy arrays that need to be transferred\n",
    "    if isinstance(L, cp.ndarray):\n",
    "        L = cp.asnumpy(L)\n",
    "    elif hasattr(L, 'toarray') and hasattr(L, 'get'):\n",
    "        L = L.get().toarray()\n",
    "    elif hasattr(L, 'toarray'):\n",
    "        L = L.toarray()\n",
    "    \n",
    "    # Get Fiedler vector and sort vertices\n",
    "    _, eigenvecs = LA.eigsh(L, k=2, which='SA')\n",
    "    fiedler_vector = eigenvecs[:, 1]\n",
    "    sorted_args = np.argsort(fiedler_vector)\n",
    "    \n",
    "    # Reorder adjacency matrix\n",
    "    adj = -L[np.ix_(sorted_args, sorted_args)]\n",
    "    \n",
    "    # Find best cut\n",
    "    ind = np.arange(1, n)\n",
    "    upper_tri_sums = np.array([np.sum(adj[i:, :i]) for i in ind])\n",
    "    qualities = upper_tri_sums / (ind * (n - ind))\n",
    "    \n",
    "    best_cut = np.argmin(qualities) + 1\n",
    "    \n",
    "    # Get the groups based on sorted indices\n",
    "    first_group = sorted_args[:best_cut]\n",
    "    second_group = sorted_args[best_cut:]\n",
    "\n",
    "    if 0 in first_group:\n",
    "        return first_group.tolist(), second_group.tolist()\n",
    "    return second_group.tolist(), first_group.tolist()\n",
    "\n",
    "# ============================================================================\n",
    "# TREE STRUCTURE\n",
    "# ============================================================================\n",
    "class BiCutNode:\n",
    "    \"\"\"Node class for the bi-cut tree structure\"\"\"\n",
    "    def __init__(self, indices, left=None, right=None, parent=None):\n",
    "        self.indices = indices\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.left is None and self.right is None\n",
    "    \n",
    "    def get_order(self):\n",
    "        \"\"\"Get order of singleton vertices\"\"\"\n",
    "        order = []\n",
    "        \n",
    "        def collect_singletons(node):\n",
    "            if node.is_leaf():\n",
    "                order.extend(node.indices)\n",
    "            else:\n",
    "                if node.left:\n",
    "                    collect_singletons(node.left)\n",
    "                if node.right:\n",
    "                    collect_singletons(node.right)\n",
    "        \n",
    "        collect_singletons(self)\n",
    "        return order        \n",
    "    \n",
    "    def print_fancy_tree(self, prefix=\"\", is_last=True, is_root=True):\n",
    "        \"\"\"Print tree with fancy box-drawing characters\"\"\"\n",
    "        if is_root:\n",
    "            print(\"BiCut Tree Structure\")\n",
    "        \n",
    "        connector = \"├─\" if is_root else (\"└─\" if is_last else \"├─\")\n",
    "        indices_str = f\"[{', '.join(map(str, sorted(self.indices)))}]\"\n",
    "        \n",
    "        print(f\"{prefix}{connector} {indices_str}\")\n",
    "        \n",
    "        new_prefix = prefix + (\"│  \" if is_root else (\"   \" if is_last else \"│  \"))\n",
    "        children = [child for child in [self.left, self.right] if child is not None]\n",
    "        \n",
    "        for i, child in enumerate(children):\n",
    "            is_last_child = (i == len(children) - 1)\n",
    "            child.print_fancy_tree(new_prefix, is_last_child, False)\n",
    "\n",
    "# ============================================================================\n",
    "# TREE BUILDERS\n",
    "# ============================================================================\n",
    "def treebuilder_gpu_batch(laplacian_matrix, thre=None, use_gpu=True):\n",
    "    \"\"\"\n",
    "    GPU-optimized tree builder that minimizes transfers by processing in batches.\n",
    "    \"\"\"\n",
    "    n = laplacian_matrix.shape[0]\n",
    "    \n",
    "    if not use_gpu or not CUPY_AVAILABLE:\n",
    "        if VERBOSE:\n",
    "            print(\"GPU disabled by user or CuPy unavailable\")\n",
    "        return treebuilder_cpu(laplacian_matrix, thre, list(range(n)))\n",
    "    \n",
    "    if n < GPU_MIN_SIZE:\n",
    "        if VERBOSE:\n",
    "            print(f\"Matrix too small for GPU ({n} < {GPU_MIN_SIZE})\")\n",
    "        return treebuilder_cpu(laplacian_matrix, thre, list(range(n)))\n",
    "    \n",
    "    if VERBOSE:\n",
    "        print(f\"Using GPU batch processing for matrix size {n}x{n}\")\n",
    "    \n",
    "    # Transfer matrix to GPU once at the beginning\n",
    "    L_gpu = gpu_manager.get_gpu_matrix(laplacian_matrix, key='main_matrix')\n",
    "    \n",
    "    def build_subtree_gpu(gpu_matrix, indices, depth=0):\n",
    "        \"\"\"Recursive function that works primarily on GPU\"\"\"\n",
    "        n_sub = len(indices)\n",
    "        \n",
    "        # Base cases\n",
    "        if n_sub == 0:\n",
    "            raise ValueError(\"Empty matrix\")\n",
    "        if n_sub == 1:\n",
    "            return BiCutNode(indices)\n",
    "        if n_sub == 2:\n",
    "            return BiCutNode(indices, BiCutNode([indices[0]]), BiCutNode([indices[1]]))\n",
    "        if thre is not None and n_sub <= thre:\n",
    "            return BiCutNode(indices)\n",
    "        \n",
    "        # For large submatrices at shallow depth, use GPU\n",
    "        gpu_threshold = max(30, GPU_MIN_SIZE // (2 ** depth))\n",
    "        should_use_gpu_here = n_sub >= gpu_threshold and depth < 6\n",
    "        \n",
    "        if should_use_gpu_here:\n",
    "            try:\n",
    "                # Create submatrix indices on GPU\n",
    "                sub_indices = cp.arange(n_sub)\n",
    "                \n",
    "                # Extract submatrix (stays on GPU)\n",
    "                submatrix_gpu = gpu_matrix[cp.ix_(sub_indices, sub_indices)]\n",
    "                \n",
    "                # Process on GPU using our GPU-native function\n",
    "                first_local, second_local = bicut_group_gpu_native(submatrix_gpu)\n",
    "                \n",
    "                # Convert back to global indices\n",
    "                first_group = [indices[i] for i in first_local]\n",
    "                second_group = [indices[i] for i in second_local]\n",
    "                \n",
    "                if not second_group:\n",
    "                    return BiCutNode(indices)\n",
    "                \n",
    "                # Create node and recurse\n",
    "                node = BiCutNode(indices)\n",
    "                \n",
    "                # Create submatrices for recursion (stay on GPU)\n",
    "                first_sub_gpu = gpu_matrix[cp.ix_(cp.asarray(first_local), cp.asarray(first_local))]\n",
    "                second_sub_gpu = gpu_matrix[cp.ix_(cp.asarray(second_local), cp.asarray(second_local))]\n",
    "                \n",
    "                node.left = build_subtree_gpu(first_sub_gpu, first_group, depth + 1)\n",
    "                node.right = build_subtree_gpu(second_sub_gpu, second_group, depth + 1)\n",
    "                \n",
    "                return node\n",
    "                \n",
    "            except Exception as e:\n",
    "                if VERBOSE:\n",
    "                    print(f\"GPU processing failed at depth {depth}: {e}\")\n",
    "                # Transfer submatrix to CPU and continue\n",
    "                submatrix_cpu = cp.asnumpy(gpu_matrix[:n_sub, :n_sub])\n",
    "                return treebuilder_cpu(submatrix_cpu, thre, indices)\n",
    "        \n",
    "        else:\n",
    "            # For small submatrices, transfer to CPU\n",
    "            submatrix_cpu = cp.asnumpy(gpu_matrix[:n_sub, :n_sub])\n",
    "            return treebuilder_cpu(submatrix_cpu, thre, indices)\n",
    "    \n",
    "    try:\n",
    "        result = build_subtree_gpu(L_gpu, list(range(n)))\n",
    "        if VERBOSE:\n",
    "            stats = gpu_manager.get_stats()\n",
    "            print(f\"GPU processing complete. Transfers: {stats['transfers']}, Data: {stats['mb']:.1f}MB\")\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        if VERBOSE:\n",
    "            print(f\"GPU batch processing failed: {e}\")\n",
    "        gpu_manager.clear_cache()\n",
    "        return treebuilder_cpu(laplacian_matrix, thre, list(range(n)))\n",
    "\n",
    "def treebuilder_cpu(laplacian_matrix, thre=None, indices=None):\n",
    "    \"\"\"CPU-only tree builder (original algorithm)\"\"\"\n",
    "    if indices is None:\n",
    "        indices = list(range(laplacian_matrix.shape[0]))\n",
    "    \n",
    "    n = len(indices)\n",
    "    \n",
    "    if n == 0:\n",
    "        raise ValueError(\"Empty matrix\")\n",
    "    if n == 1:\n",
    "        return BiCutNode(indices)\n",
    "    if n == 2:\n",
    "        return BiCutNode(indices, BiCutNode([indices[0]]), BiCutNode([indices[1]]))\n",
    "    if thre is not None and n <= thre:\n",
    "        return BiCutNode(indices)\n",
    "    \n",
    "    # Extract submatrix\n",
    "    submatrix = laplacian_matrix[np.ix_(range(n), range(n))]\n",
    "    \n",
    "    # Apply bicut - explicitly disable GPU to avoid recursion issues\n",
    "    first_group_local, second_group_local = bicut_group(submatrix, use_gpu=False)\n",
    "    \n",
    "    # Convert to global indices\n",
    "    first_group = [indices[i] for i in first_group_local]\n",
    "    second_group = [indices[i] for i in second_group_local]\n",
    "    \n",
    "    if not second_group:\n",
    "        return BiCutNode(indices)\n",
    "    \n",
    "    # Create node and recurse\n",
    "    node = BiCutNode(indices)\n",
    "    \n",
    "    first_submatrix = laplacian_matrix[np.ix_(first_group_local, first_group_local)]\n",
    "    second_submatrix = laplacian_matrix[np.ix_(second_group_local, second_group_local)]\n",
    "    \n",
    "    node.left = treebuilder_cpu(first_submatrix, thre, first_group)\n",
    "    node.right = treebuilder_cpu(second_submatrix, thre, second_group)\n",
    "    \n",
    "    return node\n",
    "\n",
    "def treebuilder(laplacian_matrix, thre=None, indices=None, use_gpu=False):\n",
    "    \"\"\"\n",
    "    Main treebuilder function with transfer-optimized GPU acceleration.\n",
    "    \"\"\"\n",
    "    # Clear any previous GPU cache\n",
    "    gpu_manager.clear_cache()\n",
    "    \n",
    "    n = laplacian_matrix.shape[0]\n",
    "    if VERBOSE:\n",
    "        print(f\"Building tree for {n}x{n} matrix...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    if use_gpu and CUPY_AVAILABLE:\n",
    "        result = treebuilder_gpu_batch(laplacian_matrix, thre)\n",
    "    else:\n",
    "        result = treebuilder_cpu(laplacian_matrix, thre, indices)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    if VERBOSE:\n",
    "        print(f\"Tree built in {elapsed:.3f}s\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ============================================================================\n",
    "# STRUCTURED MATRIX GENERATION\n",
    "# ============================================================================\n",
    "def generate_layers_groups_graph(num_supergroups=3, \n",
    "                                 num_subgroups_per_supergroup=4, \n",
    "                                 nodes_per_subgroup=10,\n",
    "                                 p_intra_subgroup=0.8,\n",
    "                                 p_intra_supergroup=0.3,\n",
    "                                 p_inter_supergroup=0.05,\n",
    "                                 seed=None):\n",
    "    \"\"\"\n",
    "    Generate a hierarchical graph with super-groups and sub-groups structure.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    total_subgroups = num_supergroups * num_subgroups_per_supergroup\n",
    "    total_nodes = total_subgroups * nodes_per_subgroup\n",
    "    \n",
    "    # Initialize adjacency matrix\n",
    "    adj = np.zeros((total_nodes, total_nodes))\n",
    "    \n",
    "    # Create edges\n",
    "    for i in range(total_nodes):\n",
    "        for j in range(i + 1, total_nodes):\n",
    "            # Determine which groups nodes belong to\n",
    "            subgroup_i = i // nodes_per_subgroup\n",
    "            subgroup_j = j // nodes_per_subgroup\n",
    "            supergroup_i = subgroup_i // num_subgroups_per_supergroup\n",
    "            supergroup_j = subgroup_j // num_subgroups_per_supergroup\n",
    "            \n",
    "            # Determine edge probability\n",
    "            if subgroup_i == subgroup_j:\n",
    "                p = p_intra_subgroup\n",
    "            elif supergroup_i == supergroup_j:\n",
    "                p = p_intra_supergroup\n",
    "            else:\n",
    "                p = p_inter_supergroup\n",
    "            \n",
    "            # Create edge with probability p\n",
    "            if np.random.random() < p:\n",
    "                adj[i, j] = 1\n",
    "                adj[j, i] = 1\n",
    "    \n",
    "    # Compute Laplacian\n",
    "    degrees = np.sum(adj, axis=1)\n",
    "    L = np.diag(degrees) - adj\n",
    "    \n",
    "    return L\n",
    "\n",
    "def generate_block_diagonal_graph(block_sizes, p_intra=0.8, p_inter=0.05, seed=None):\n",
    "    \"\"\"\n",
    "    Generate a graph with block diagonal structure.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    n = sum(block_sizes)\n",
    "    adj = np.zeros((n, n))\n",
    "    \n",
    "    # Determine block boundaries\n",
    "    block_starts = [0]\n",
    "    for size in block_sizes[:-1]:\n",
    "        block_starts.append(block_starts[-1] + size)\n",
    "    \n",
    "    # Create edges\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            # Find which blocks i and j belong to\n",
    "            block_i = 0\n",
    "            block_j = 0\n",
    "            for k, start in enumerate(block_starts):\n",
    "                if i >= start:\n",
    "                    block_i = k\n",
    "                if j >= start:\n",
    "                    block_j = k\n",
    "            \n",
    "            # Set edge probability\n",
    "            p = p_intra if block_i == block_j else p_inter\n",
    "            \n",
    "            if np.random.random() < p:\n",
    "                adj[i, j] = 1\n",
    "                adj[j, i] = 1\n",
    "    \n",
    "    # Compute Laplacian\n",
    "    degrees = np.sum(adj, axis=1)\n",
    "    L = np.diag(degrees) - adj\n",
    "    \n",
    "    return L\n",
    "\n",
    "def generate_random_graph(n, density=0.1, seed=None):\n",
    "    \"\"\"\n",
    "    Generate a random Erdős–Rényi graph.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    adj = np.random.random((n, n)) < density\n",
    "    adj = adj.astype(float)\n",
    "    adj = (adj + adj.T) / 2\n",
    "    np.fill_diagonal(adj, 0)\n",
    "    \n",
    "    degrees = np.sum(adj, axis=1)\n",
    "    L = np.diag(degrees) - adj\n",
    "    \n",
    "    return L\n",
    "\n",
    "# ============================================================================\n",
    "# PERFORMANCE TESTING\n",
    "# ============================================================================\n",
    "def test_duration_memory(laplacian_matrix, thre=None, use_gpu=False):\n",
    "    \"\"\"\n",
    "    Test the duration and memory usage of treebuilder.\n",
    "    \"\"\"\n",
    "    # Clear caches\n",
    "    gc.collect()\n",
    "    if CUPY_AVAILABLE:\n",
    "        gpu_manager.clear_cache()\n",
    "        if use_gpu:\n",
    "            cp.get_default_memory_pool().free_all_blocks()\n",
    "    \n",
    "    # Start monitoring\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run treebuilder\n",
    "    result = treebuilder(laplacian_matrix, thre=thre, use_gpu=use_gpu)\n",
    "    \n",
    "    # Get metrics\n",
    "    duration = time.time() - start_time\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    \n",
    "    peak_memory_mb = peak / (1024 * 1024)\n",
    "    memory_ratio = current / peak if peak > 0 else 0\n",
    "    \n",
    "    return duration, peak_memory_mb, memory_ratio\n",
    "\n",
    "def parallel_choices_test(iter_count=10, sup=2, sub=3, node=5, test_gpu=True):\n",
    "    \"\"\"\n",
    "    Test performance with different matrix structures and parameters.\n",
    "    \"\"\"\n",
    "    import csv\n",
    "    \n",
    "    # Prepare CSV file\n",
    "    csv_filename = 'parallel_test_results.csv'\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"STRUCTURED MATRIX PERFORMANCE TEST\")\n",
    "    print(f\"Results will be saved to: {csv_filename}\")\n",
    "    print(f\"Testing {'CPU and GPU' if test_gpu and CUPY_AVAILABLE else 'CPU only'}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    with open(csv_filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        # Write header\n",
    "        header = ['iteration', 'matrix_type', 'total_nodes', 'structure',\n",
    "                  'cpu_duration_thre', 'cpu_memory_thre', 'cpu_ratio_thre', \n",
    "                  'cpu_duration_no_thre', 'cpu_memory_no_thre', 'cpu_ratio_no_thre']\n",
    "        \n",
    "        if test_gpu and CUPY_AVAILABLE:\n",
    "            header.extend(['gpu_duration_thre', 'gpu_memory_thre', 'gpu_ratio_thre',\n",
    "                          'gpu_duration_no_thre', 'gpu_memory_no_thre', 'gpu_ratio_no_thre',\n",
    "                          'speedup_thre', 'speedup_no_thre'])\n",
    "        \n",
    "        writer.writerow(header)\n",
    "        \n",
    "        for i in range(1, iter_count + 1):\n",
    "            total_nodes = i * sup * sub * node\n",
    "            print(f\"\\nIteration {i}/{iter_count} (total nodes: {total_nodes})\")\n",
    "            \n",
    "            # Test different matrix structures\n",
    "            test_configs = [\n",
    "                ('supergroups', i*sup, sub, node, f\"{i*sup}x{sub}x{node}\"),\n",
    "                ('subgroups', sup, i*sub, node, f\"{sup}x{i*sub}x{node}\"),\n",
    "                ('nodes_per_sub', sup, sub, i*node, f\"{sup}x{sub}x{i*node}\")\n",
    "            ]\n",
    "            \n",
    "            for matrix_type, s, sb, n, structure in test_configs:\n",
    "                print(f\"  Testing {matrix_type}: {structure}\")\n",
    "                \n",
    "                # Generate structured matrix\n",
    "                L = generate_layers_groups_graph(\n",
    "                    num_supergroups=s,\n",
    "                    num_subgroups_per_supergroup=sb,\n",
    "                    nodes_per_subgroup=n,\n",
    "                    p_intra_subgroup=0.8,\n",
    "                    p_intra_supergroup=0.3,\n",
    "                    p_inter_supergroup=0.05,\n",
    "                    seed=42\n",
    "                )\n",
    "                \n",
    "                total = s * sb * n\n",
    "                thre_value = n if matrix_type == 'nodes_per_sub' else node\n",
    "                \n",
    "                # Test CPU version\n",
    "                print(f\"    CPU tests...\", end='')\n",
    "                cpu_thre = test_duration_memory(L, thre=thre_value, use_gpu=False)\n",
    "                cpu_no_thre = test_duration_memory(L, thre=1, use_gpu=False)\n",
    "                print(f\" done (thre={thre_value}: {cpu_thre[0]:.2f}s, thre=1: {cpu_no_thre[0]:.2f}s)\")\n",
    "                \n",
    "                row = [i, matrix_type, total, structure,\n",
    "                       cpu_thre[0], cpu_thre[1], cpu_thre[2],\n",
    "                       cpu_no_thre[0], cpu_no_thre[1], cpu_no_thre[2]]\n",
    "                \n",
    "                # Test GPU version if available\n",
    "                if test_gpu and CUPY_AVAILABLE and total >= GPU_MIN_SIZE:\n",
    "                    print(f\"    GPU tests...\", end='')\n",
    "                    gpu_thre = test_duration_memory(L, thre=thre_value, use_gpu=True)\n",
    "                    gpu_no_thre = test_duration_memory(L, thre=1, use_gpu=True)\n",
    "                    \n",
    "                    speedup_thre = cpu_thre[0] / gpu_thre[0] if gpu_thre[0] > 0 else 0\n",
    "                    speedup_no_thre = cpu_no_thre[0] / gpu_no_thre[0] if gpu_no_thre[0] > 0 else 0\n",
    "                    \n",
    "                    print(f\" done (speedup: {speedup_thre:.2f}x with thre, {speedup_no_thre:.2f}x without)\")\n",
    "                    \n",
    "                    row.extend([gpu_thre[0], gpu_thre[1], gpu_thre[2],\n",
    "                               gpu_no_thre[0], gpu_no_thre[1], gpu_no_thre[2],\n",
    "                               speedup_thre, speedup_no_thre])\n",
    "                elif test_gpu and CUPY_AVAILABLE:\n",
    "                    print(f\"    GPU skipped (matrix too small: {total} < {GPU_MIN_SIZE})\")\n",
    "                    row.extend([None] * 8)\n",
    "                \n",
    "                writer.writerow(row)\n",
    "                csvfile.flush()\n",
    "            \n",
    "            if i % 5 == 0:\n",
    "                print(f\"  Completed {i}/{iter_count} iterations\")\n",
    "    \n",
    "    print(f\"\\nTest completed! Results saved to {csv_filename}\")\n",
    "\n",
    "# ============================================================================\n",
    "# BENCHMARKING\n",
    "# ============================================================================\n",
    "def benchmark_transfer_analysis(laplacian_matrix, thre=None, num_runs=3):\n",
    "    \"\"\"\n",
    "    Benchmark with detailed transfer analysis.\n",
    "    \"\"\"\n",
    "    n = laplacian_matrix.shape[0]\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TRANSFER ANALYSIS BENCHMARK - Matrix {n}x{n}\")\n",
    "    print(f\"GPU_MIN_SIZE threshold: {GPU_MIN_SIZE}\")\n",
    "    print(f\"CuPy available: {CUPY_AVAILABLE}\")\n",
    "    print(f\"Matrix qualifies for GPU: {n >= GPU_MIN_SIZE}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # CPU benchmark\n",
    "    print(\"\\nCPU VERSION:\")\n",
    "    cpu_times = []\n",
    "    for i in range(num_runs):\n",
    "        gpu_manager.clear_cache()\n",
    "        print(f\"  CPU Run {i+1}...\", end='')\n",
    "        start = time.time()\n",
    "        cpu_result = treebuilder(laplacian_matrix, thre=thre, use_gpu=False)\n",
    "        end = time.time()\n",
    "        cpu_times.append(end - start)\n",
    "        print(f\" {cpu_times[-1]:.3f}s\")\n",
    "    \n",
    "    cpu_avg = np.mean(cpu_times)\n",
    "    print(f\"CPU Average: {cpu_avg:.3f}s\")\n",
    "    \n",
    "    # GPU benchmark if available\n",
    "    if CUPY_AVAILABLE and n >= GPU_MIN_SIZE:\n",
    "        print(f\"\\nGPU VERSION:\")\n",
    "        \n",
    "        gpu_times = []\n",
    "        transfer_stats = []\n",
    "        \n",
    "        for i in range(num_runs):\n",
    "            gpu_manager.clear_cache()\n",
    "            gpu_manager.transfer_count = 0\n",
    "            gpu_manager.transfer_bytes = 0\n",
    "            \n",
    "            print(f\"  GPU Run {i+1}...\", end='')\n",
    "            start = time.time()\n",
    "            gpu_result = treebuilder(laplacian_matrix, thre=thre, use_gpu=True)\n",
    "            cp.cuda.Stream.null.synchronize()\n",
    "            end = time.time()\n",
    "            \n",
    "            current_stats = gpu_manager.get_stats()\n",
    "            \n",
    "            gpu_times.append(end - start)\n",
    "            transfer_stats.append(current_stats.copy())\n",
    "            \n",
    "            print(f\" {gpu_times[-1]:.3f}s, Transfers: {transfer_stats[-1]['transfers']}, Data: {transfer_stats[-1]['mb']:.1f}MB\")\n",
    "        \n",
    "        gpu_avg = np.mean(gpu_times)\n",
    "        avg_transfers = np.mean([s['transfers'] for s in transfer_stats])\n",
    "        avg_mb = np.mean([s['mb'] for s in transfer_stats])\n",
    "        \n",
    "        print(f\"GPU Average: {gpu_avg:.3f}s\")\n",
    "        print(f\"Avg Transfers: {avg_transfers:.1f}\")\n",
    "        print(f\"Avg Data: {avg_mb:.1f}MB\")\n",
    "        \n",
    "        speedup = cpu_avg / gpu_avg if gpu_avg > 0 else 0\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"ANALYSIS:\")\n",
    "        print(f\"  Speedup: {speedup:.2f}x\")\n",
    "        \n",
    "        if avg_transfers == 0:\n",
    "            print(\"  WARNING: No GPU transfers detected!\")\n",
    "        else:\n",
    "            print(f\"  GPU successfully used: {avg_transfers:.0f} transfers, {avg_mb:.1f}MB\")\n",
    "        \n",
    "        # Verify correctness\n",
    "        try:\n",
    "            cpu_order = cpu_result.get_order()\n",
    "            gpu_order = gpu_result.get_order()\n",
    "            identical = cpu_order == gpu_order\n",
    "            print(f\"  Results identical: {identical}\")\n",
    "            if not identical:\n",
    "                print(\"  WARNING: GPU and CPU results differ!\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Could not verify results: {e}\")\n",
    "        \n",
    "        results = {\n",
    "            'cpu_time': cpu_avg,\n",
    "            'gpu_time': gpu_avg,\n",
    "            'speedup': speedup,\n",
    "            'transfers': avg_transfers,\n",
    "            'transfer_mb': avg_mb,\n",
    "            'gpu_actually_used': avg_transfers > 0\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        print(f\"\\nGPU not available or matrix too small\")\n",
    "        results = {'cpu_time': cpu_avg, 'gpu_time': None}\n",
    "    \n",
    "    gpu_manager.clear_cache()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "250f4abb-e265-4582-aea9-8c2c72813cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRANSFER-OPTIMIZED BICUT WITH CUPY\n",
      "================================================================================\n",
      "\n",
      "DEMO: STRUCTURED MATRIX TESTS\n",
      "------------------------------------------------------------\n",
      "\n",
      "Test 1: Hierarchical Community Structure\n",
      "  Structure: 2 supergroups x 3 subgroups x 20 nodes\n",
      "  Matrix size: 2000x2000\n",
      "  CPU Performance: Time: 2.447s, Memory: 168.9MB\n",
      "  GPU Performance: Time: 4.413s, Memory: 9.1MB, Speedup: 0.55x\n",
      "\n",
      "Running benchmark with larger matrix...\n",
      "\n",
      "======================================================================\n",
      "TRANSFER ANALYSIS BENCHMARK - Matrix 500x500\n",
      "GPU_MIN_SIZE threshold: 100\n",
      "CuPy available: True\n",
      "Matrix qualifies for GPU: True\n",
      "======================================================================\n",
      "\n",
      "CPU VERSION:\n",
      "  CPU Run 1... 0.455s\n",
      "  CPU Run 2... 0.451s\n",
      "CPU Average: 0.453s\n",
      "\n",
      "GPU VERSION:\n",
      "  GPU Run 1... 0.749s, Transfers: 1, Data: 1.9MB\n",
      "  GPU Run 2... 0.651s, Transfers: 1, Data: 1.9MB\n",
      "GPU Average: 0.700s\n",
      "Avg Transfers: 1.0\n",
      "Avg Data: 1.9MB\n",
      "\n",
      "==================================================\n",
      "ANALYSIS:\n",
      "  Speedup: 0.65x\n",
      "  GPU successfully used: 1 transfers, 1.9MB\n",
      "  Results identical: False\n",
      "  WARNING: GPU and CPU results differ!\n",
      "\n",
      "============================================================\n",
      "Usage:\n",
      "  set_verbose(True)  # Enable verbose output\n",
      "  set_verbose(False) # Disable verbose output (default)\n",
      "  \n",
      "  # Run parallel test:\n",
      "  parallel_choices_test(iter_count=10)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*80)\n",
    "    print(\"TRANSFER-OPTIMIZED BICUT WITH CUPY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Quick demo\n",
    "    print(\"\\nDEMO: STRUCTURED MATRIX TESTS\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Test hierarchical community structure\n",
    "    print(\"\\nTest 1: Hierarchical Community Structure\")\n",
    "    print(\"  Structure: 2 supergroups x 3 subgroups x 20 nodes\")\n",
    "    L_hier = generate_layers_groups_graph(\n",
    "        num_supergroups=10,\n",
    "        num_subgroups_per_supergroup=1,\n",
    "        nodes_per_subgroup=20,\n",
    "        p_intra_subgroup=0.8,\n",
    "        p_intra_supergroup=0.3,\n",
    "        p_inter_supergroup=0.05,\n",
    "        seed=42\n",
    "    )\n",
    "    print(f\"  Matrix size: {L_hier.shape[0]}x{L_hier.shape[0]}\")\n",
    "    \n",
    "    print(\"  CPU Performance:\", end='')\n",
    "    cpu_time, cpu_mem, _ = test_duration_memory(L_hier, thre=10, use_gpu=False)\n",
    "    print(f\" Time: {cpu_time:.3f}s, Memory: {cpu_mem:.1f}MB\")\n",
    "    \n",
    "    if CUPY_AVAILABLE and L_hier.shape[0] >= GPU_MIN_SIZE:\n",
    "        print(\"  GPU Performance:\", end='')\n",
    "        gpu_time, gpu_mem, _ = test_duration_memory(L_hier, thre=10, use_gpu=True)\n",
    "        print(f\" Time: {gpu_time:.3f}s, Memory: {gpu_mem:.1f}MB, Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "    \n",
    "    # Run benchmark\n",
    "    print(\"\\nRunning benchmark with larger matrix...\")\n",
    "    L_test = generate_random_graph(500, density=0.05, seed=42)\n",
    "    results = benchmark_transfer_analysis(L_test, thre=10, num_runs=2)\n",
    "    \n",
    "    gpu_manager.clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e86564e-3035-4eaf-ad89-b804e5484570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38572885-a64d-40e4-a363-407450b269fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
